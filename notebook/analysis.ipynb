{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from datetime import datetime\n",
    "from os import path\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import trange\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from tabular.algo import model_based\n",
    "from tabular.algo.model_based import ModelBased\n",
    "from tabular.finite_mdp import FiniteMDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Finite-horizon MDP\")\n",
    "parser.add_argument(\"--n-episode\", type=int, default=1000)\n",
    "parser.add_argument(\"--n-action\", type=int, default=2)\n",
    "parser.add_argument(\"--n-step\", type=int, default=2)\n",
    "parser.add_argument(\"--state-per-stage\", type=int, default=2)\n",
    "parser.add_argument(\"--p\", type=int, default=0.05)\n",
    "parser.add_argument(\"--alpha\", type=int, default=0.1)\n",
    "parser.add_argument(\"--n-run\", type=int, default=10)\n",
    "parser.add_argument(\"--n-pol-eval-step\", type=int, default=1)\n",
    "parser.add_argument(\"--c\", type=float, default=0.1)\n",
    "parser.add_argument(\"--random-reward\", action=\"store_true\", default=True)\n",
    "args = parser.parse_args('--n-episode 1000 --n-action 4 --n-step 2 --state-per-stage 2 --c 0.1 --n-run 1'.split())\n",
    "setting = vars(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp(prev_estimate):\n",
    "    env = FiniteMDP(setting)\n",
    "\n",
    "    regret_df = []\n",
    "    step = int(max(setting[\"n_episode\"] / 100, 1))\n",
    "    episode_index = np.arange(start=0, stop=setting[\"n_episode\"], step=step)\n",
    "    algorithm = ModelBased(algorithm_type=model_based.POLICY_ITERATION, using_previous_estimate=prev_estimate)\n",
    "    regret, info = algorithm.run(setting['c'], setting, env)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(info['delta'])\n",
    "    plt.axhline(y=0)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(np.cumsum(regret))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# previous estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(4):\n",
    "    exp(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# same estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(4):\n",
    "    exp(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
